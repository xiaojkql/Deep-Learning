### Deep-Learning
人工神经元：感知机和S型神经元
人工神经网络学习算法--随机梯度下降算法

### 感知机
决策模型
感知机网络模型
多层的感知机网络--作出复杂巧妙的决策
阀值，偏置，激活感知机
权衡依据进而作出决策

出发点：设置算法，自动学习感知机网络的权重和偏置项

因为感知机的输入与输出是二值的，要么为零要么为一，不是连续的变化的，所以在操作起来会变的很困难。
感知机，不是连续函数，任何对权重偏置的改变不会引起输出的微小连续变化，而是瞬间变化或者没有变化

### S型神经元

学习：反复改变权重与偏置，以是的获更加好的输出！

我们需要微小的变化，以致输出结果不会翻转的过大，学习算法不会变的太复杂
所以输入与输出是可以取[0,1]之间的任意值的。

S型函数
代数形式上具有很多的技术细节
sigmoid 函数
为什么要使用这种形式的函数呢？
逻辑函数，逻辑神经元

平滑的感知机

由一阶微分得到一个关于权重与偏置项变化的线性函数-----目标与变量之间变化的关系，而相反感知机是无法得到这样的一个具有具体形式的函数。

使用一个约定来解释神经元的输出。二不仅仅是二值了。


### 神经网络的架构
术语部分：
输入层，输入神经元
输出层，输出神经元
中间层，隐藏层
多层感知机 MLP
输入输出隐藏层的设计

前馈神经网络---> 网络中没有回路
向前传播，反向反馈
循环神经网络-->级联的神经元激活系统


### 一个简单的分类手写数字的网络
设计神经网络
灰度级像素

"激活"的意思就是最终取的是哪一个输出神经元你的值
激活值

“启发性的方法”告诉我们怎样设计神经网络

隐藏层的神经元被激活


### 使用梯度下降算法进行学习
神经网络的设计
代价函数，损失，目标函数
二次代价函数，均方误差，MSE
训练算法的目的，最小化权重，偏置的二次代价函数
“平滑的函数”
平滑很容易解决，如何改变自变量而带来因变量的变化
学习速率
变量更新规则-->梯度下降算法
梯度下降的变化形式
寻找梯度下降算法的替代品
随机梯度下降法-->随机选取小量的训练样本进行训练-->对实际梯度的一种估算方法
小批量数据
迭代期
在线，online,on-line,递增学习


### 实现网络
超参数



## 反向传播算法

代价函数的梯度
反向传播
---改变权重和偏置时，代价函数变化的大小

### 神经网络中使用矩阵快速计算输出的方法
偏置
激活值
中间量：带权输入

误差的度量
四个基本方程
