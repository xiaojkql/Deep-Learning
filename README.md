### Deep-Learning
人工神经元：感知机和S型神经元
人工神经网络学习算法--随机梯度下降算法

### 感知机
决策模型
感知机网络模型
多层的感知机网络--作出复杂巧妙的决策
阀值，偏置，激活感知机
权衡依据进而作出决策

出发点：设置算法，自动学习感知机网络的权重和偏置项

因为感知机的输入与输出是二值的，要么为零要么为一，不是连续的变化的，所以在操作起来会变的很困难。
感知机，不是连续函数，任何对权重偏置的改变不会引起输出的微小连续变化，而是瞬间变化或者没有变化

### S型神经元

学习：反复改变权重与偏置，以是的获更加好的输出！

我们需要微小的变化，以致输出结果不会翻转的过大，学习算法不会变的太复杂
所以输入与输出是可以取[0,1]之间的任意值的。

S型函数
代数形式上具有很多的技术细节
sigmoid 函数
为什么要使用这种形式的函数呢？
逻辑函数，逻辑神经元

平滑的感知机

由一阶微分得到一个关于权重与偏置项变化的线性函数-----目标与变量之间变化的关系，而相反感知机是无法得到这样的一个具有具体形式的函数。

使用一个约定来解释神经元的输出。二不仅仅是二值了。


### 神经网络的架构
术语部分：
输入层，输入神经元
输出层，输出神经元
中间层，隐藏层
多层感知机 MLP
输入输出隐藏层的设计

前馈神经网络---> 网络中没有回路
向前传播，反向反馈
循环神经网络-->级联的神经元激活系统


### 一个简单的分类手写数字的网络
设计神经网络
灰度级像素

"激活"的意思就是最终取的是哪一个输出神经元你的值
激活值

“启发性的方法”告诉我们怎样设计神经网络

隐藏层的神经元被激活


### 使用梯度下降算法进行学习
神经网络的设计
代价函数，损失，目标函数
二次代价函数，均方误差，MSE
训练算法的目的，最小化权重，偏置的二次代价函数
“平滑的函数”
平滑很容易解决，如何改变自变量而带来因变量的变化
学习速率
变量更新规则-->梯度下降算法
梯度下降的变化形式
寻找梯度下降算法的替代品
随机梯度下降法-->随机选取小量的训练样本进行训练-->对实际梯度的一种估算方法
小批量数据
迭代期
在线，online,on-line,递增学习


### 实现网络
超参数

### 在那种层面上，反向传播是快速的算法？
现代计算机的计算能力+新的想法赋予反向传播训练深度网络

### 反向传播：全局观




## 反向传播算法

代价函数的梯度
反向传播
---改变权重和偏置时，代价函数变化的大小

### 神经网络中使用矩阵快速计算输出的方法
偏置
激活值
中间量：带权输入

误差的度量
四个基本方程

变化率因子，路径的变化率因子
网络行为
计算所有可能的路径变化率和的方式
构造的细节

## 改进神经网络学习方法
技术

交叉熵代价函数
正则化方法：L1 L2 弃权 训练数据的认为扩展 (四种) --> 目的在其他数据集上表现的更好
更好的权重初始化方法
选择超参数的启发式方法

### 交叉熵代价函数
更好的定义错误，更好的学习，更快的学习
神经网络从错误中学习
犯错比较明显的时候学习的速度更快
但是神经网络确出现了异常，在更坏的情况下，学习速度竟然如此慢
神经网络学习慢-->偏导数很小
神经网络学习速速下降的原因具有普适性

#### 引入交叉熵代价函数 替换二次代价函数
交叉熵：一是非负的，二是离目标值越近那么交叉熵越接近于零(正确零很好的时候，交叉熵接近于零)
同时避免了学习速度下降的问题
即，权重的学习的速度受到输出中误差的影响，误差越大，学习速度越快
交叉熵仅仅是一种选择而已

不同的代价函数，使用的学习速率会是不一样的

学习速率与代价函数有关，而与学习速率没有很大的关系

当输出神经元是S型神经元时，使用交叉熵会是一种明智的选择

神经元饱和

